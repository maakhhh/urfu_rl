{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnjcn5TYn0BG",
        "outputId": "1af88e4c-11e7-44e8-eb25-521be42e2306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/953.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m788.5/953.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n"
      ],
      "metadata": {
        "id": "dOeB6hzwozbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Фичи, которые будет менять агент: Flow Duration, Fwd Pkt Len Mean, Bwd Pkt Len Mean, Tot Fwd Pkts, Tot Bwd Pkts"
      ],
      "metadata": {
        "id": "QGGWIamjaRIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Detector:\n",
        "    def __init__(self, classifier):\n",
        "        self.classifier = classifier;\n",
        "\n",
        "    def predict(self, x):\n",
        "        y = self.classifier.predict(x);\n",
        "        return y\n",
        "\n",
        "    def fit(self, data):\n",
        "        x = data.drop('Label', axis=1)\n",
        "        y = data['Label']\n",
        "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "        self.classifier.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "UEC8LUNGlWQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piVVyFvQndYS"
      },
      "outputs": [],
      "source": [
        "class Enviroment(gym.Env):\n",
        "    def __init__(self, detector, attack_traffic):\n",
        "        super().__init__()\n",
        "        self.detector = detector\n",
        "        self.start = attack_traffic\n",
        "        self.state = attack_traffic\n",
        "        self.space = [{\"Name\": \"Flow Duration\", \"Action\": \"+Flow Duration\", \"Value\": 100},\n",
        "                      {\"Name\": \"Flow Duration\", \"Action\": \"-Flow Duration\", \"Value\": -100},\n",
        "                      {\"Name\": \"Fwd Pkt Len Mean\", \"Action\": \"+Fwd Pkt Len Mean\", \"Value\": 0.1},\n",
        "                      {\"Name\": \"Fwd Pkt Len Mean\", \"Action\": \"-Fwd Pkt Len Mean\", \"Value\": -0.1},\n",
        "                      {\"Name\": \"Bwd Pkt Len Mean\", \"Action\": \"+Bwd Pkt Len Mean\", \"Value\": 0.1},\n",
        "                      {\"Name\": \"Bwd Pkt Len Mean\", \"Action\": \"-Bwd Pkt Len Mean\", \"Value\": -0.1},\n",
        "                      {\"Name\": \"Tot Fwd Pkts\", \"Action\": \"+Tot Fwd Pkts\", \"Value\": 0.1},\n",
        "                      {\"Name\": \"Tot Fwd Pkts\", \"Action\": \"-Tot Fwd Pkts\", \"Value\": -0.1},\n",
        "                      {\"Name\": \"Tot Bwd Pkts\", \"Action\": \"+Tot Bwd Pkts\", \"Value\": 0.1},\n",
        "                      {\"Name\": \"Tot Bwd Pkts\", \"Action\": \"-Tot Bwd Pkts\", \"Value\": -0.1}]\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action_n):\n",
        "        action = self.space[action_n]\n",
        "        self.state[action[\"Name\"]] += action[\"Value\"]\n",
        "        result = self.detector.predict(self.state)\n",
        "        return (self.state, 1 if result == 0 else 1, False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CEM():\n",
        "    def __init__(self, state_n, action_n):\n",
        "      self.state_n = state_n\n",
        "      self.action_n = action_n\n",
        "      self.policy = np.ones((self.state_n, self.action_n)) / self.action_n\n",
        "\n",
        "    def get_action(self, state):\n",
        "      return int(np.random.choice(np.arange(self.action_n), p=self.policy[state]))\n",
        "\n",
        "    def update_policy(self, elite_tr):\n",
        "      pre_policy = np.zeros((state_n, action_n))\n",
        "\n",
        "      for tr in elite_tr:\n",
        "        for state, action in zip(tr['states'], tr['actions']):\n",
        "          pre_policy[state][action] += 1\n",
        "\n",
        "          for state in range(self.action_n):\n",
        "            if sum(pre_policy[state]) == 0:\n",
        "              self.policy[state] = np.ones(self.action_n) / self.action_n\n",
        "            else:\n",
        "              self.policy[state] = pre_policy[state] / sum(pre_policy[state])"
      ],
      "metadata": {
        "id": "Y2yfqwRcXKqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CEMAgent():\n",
        "    def __init__(self, env, agent_logic):\n",
        "      self.env = env\n",
        "      self.agent_logic = agent_logic\n",
        "\n",
        "    def get_state(self):\n",
        "      return None\n",
        "\n",
        "    def get_trajectory(self, tr_len):\n",
        "      tr = {'states': [], 'actions': [], 'total_reward': 0}\n",
        "\n",
        "      obs = self.env.reset()\n",
        "      state = get_state(obs)\n",
        "      tr['states'].append(state)\n",
        "\n",
        "      for _ in range(tr_len):\n",
        "          action = self.agent_logic.get_action(state)\n",
        "          tr['actions'].append(action)\n",
        "\n",
        "          obs, reward, done = env.step(action)\n",
        "          state = get_state(obs)\n",
        "          tr['total_reward'] += reward\n",
        "\n",
        "          if done:\n",
        "              break\n",
        "\n",
        "          tr['states'].append(state)\n",
        "\n",
        "      return tr\n",
        "\n",
        "      def get_elite_trajectories(self, trajectories, q):\n",
        "          total = [tr['total_reward'] for tr in trajectories]\n",
        "          quantile = np.quantile(total, q=q)\n",
        "          return [tr for tr in trajectories if tr['total_reward'] > quantile]\n",
        "\n",
        "      def fit(self, episode_n, trajectory_n, trajectory_len, q):\n",
        "          for _ in range(episode_n):\n",
        "              trajectories = [get_trajectory(trajectory_len) for _ in range(trajectory_n)]\n",
        "\n",
        "              mean_total = np.mean([trajectory['total_reward'] for trajectory in trajectories])\n",
        "              print(mean_total)\n",
        "              elite = get_elite_trajectories(trajectories, q)\n",
        "\n",
        "              if len(elite) > 0:\n",
        "                  self.agent_logic.update_policy(elite)"
      ],
      "metadata": {
        "id": "581vBZbWe6wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('data.csv')\n",
        "forest = RandomForestClassifier()\n",
        "detector = Detector(forest)\n",
        "detector.fit(data)"
      ],
      "metadata": {
        "id": "Vvri8PLvh_2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = Enviroment(detector, None)\n",
        "agent_logic = CEM(0, 10)\n",
        "agent = CEMAgent(env, agent_logic)\n",
        "agent.fit(70, 200, 20. 0.5)"
      ],
      "metadata": {
        "id": "laA38_Pei7o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kSOarjebbTiZ"
      }
    }
  ]
}