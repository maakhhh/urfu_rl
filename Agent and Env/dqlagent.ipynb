{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnjcn5TYn0BG",
        "outputId": "1af88e4c-11e7-44e8-eb25-521be42e2306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/953.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/953.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m788.5/953.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
        "import random\n",
        "from collections import deque\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n"
      ],
      "metadata": {
        "id": "dOeB6hzwozbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function representing the value\n",
        "of an action can be seen as a table that maps all states\n",
        "and all actions to the expected long-term return. In our\n",
        "case, the dimension of this table is large and compiling it\n",
        "requires high computational costs."
      ],
      "metadata": {
        "id": "suAhGADTYpxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Detector:\n",
        "    def __init__(self, classifier):\n",
        "        self.classifier = classifier;\n",
        "\n",
        "    def predict(self, x):\n",
        "        y = self.classifier.predict(x);\n",
        "        return y\n",
        "\n",
        "    def fit(self, data):\n",
        "        x = data.drop('Label', axis=1)\n",
        "        y = data['Label']\n",
        "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
        "        self.classifier.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "UEC8LUNGlWQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(data):\n",
        "    mean = data.mean(axis=0)\n",
        "    std = data.std(axis=0)\n",
        "    data -= mean\n",
        "    data /= std\n",
        "    return data"
      ],
      "metadata": {
        "id": "wle3bXdsKxI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piVVyFvQndYS"
      },
      "outputs": [],
      "source": [
        "class Enviroment(gym.Env):\n",
        "    def __init__(self, detector, attack_traffic):\n",
        "        super().__init__()\n",
        "        self.detector = detector\n",
        "        self.start = attack_traffic\n",
        "        self.state = attack_traffic\n",
        "        self.space = [{\"Name\": \"Flow Duration\", \"Action\": \"+Flow Duration\", \"Value\": 0.01},\n",
        "                      {\"Name\": \"Flow Duration\", \"Action\": \"-Flow Duration\", \"Value\": -0.01},\n",
        "                      {\"Name\": \"TotLen Fwd Pkts\", \"Action\": \"+TotLen Fwd Pkts\", \"Value\": 0.01},\n",
        "                      {\"Name\": \"TotLen Fwd Pkts\", \"Action\": \"-TotLen Fwd Pkts\", \"Value\": -0.01},\n",
        "                      {\"Name\": \"TotLen Bwd Pkts\", \"Action\": \"+TotLen Bwd Pkts\", \"Value\": 0.01},\n",
        "                      {\"Name\": \"TotLen Bwd Pkts\", \"Action\": \"-TotLen Bwd Pkts\", \"Value\": -0.01},\n",
        "                      {\"Name\": \"Flow Byts/s\", \"Action\": \"+Flow Byts/s\", \"Value\": 0.01},\n",
        "                      {\"Name\": \"Flow Byts/s\", \"Action\": \"-Flow Byts/s\", \"Value\": -0.01},\n",
        "                      {\"Name\": \"Flow Pkts/s\", \"Action\": \"+Flow Pkts/s\", \"Value\": 0.01},\n",
        "                      {\"Name\": \"Flow Pkts/s\", \"Action\": \"-Flow Pkts/s\", \"Value\": -0.01},\n",
        "                      {\"Name\": \"Bwd/Fwd Ratio\", \"Action\": \"+Bwd/Fwd Ratio\", \"Value\": 0.01},\n",
        "                      {\"Name\": \"Bwd/Fwd Ratio\", \"Action\": \"-Bwd/Fwd Ratio\", \"Value\": -0.01},\n",
        "                      {\"Name\": \"Pkt Size Avg\", \"Action\": \"+Pkt Size Avg\", \"Value\": 0.01},\n",
        "                      {\"Name\": \"Pkt Size Avg\", \"Action\": \"-Pkt Size Avg\", \"Value\": -0.01}]\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.start\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action_n):\n",
        "        action = self.space[action_n]\n",
        "        self.state[action[\"Name\"]] += action[\"Value\"]\n",
        "        result = self.detector.predict(self.state)\n",
        "        return (self.state, 1 if result == 0 else 0, False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQLAgent:\n",
        "    def __init__(self, state_size, action_size=state_size*2):\n",
        "        self.state_size = state_size # Сколько фич изменяем\n",
        "        self.action_size = action_size # В каком объеме меняем\n",
        "        self.memory = deque(maxlen=2000)\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.epsilon_min = 0.01\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(512, input_dim=self.state_size, activation='relu'))\n",
        "        model.add(Dense(256, activation='relu'))\n",
        "        model.add(Dense(self.action_size, activation='linear'))\n",
        "        opt = keras.optimizers.Adam(use_ema=True)\n",
        "        model.compile(loss='mse', optimizer=opt)\n",
        "        return model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.argmax(act_values[0])\n",
        "\n",
        "    def replay(self, batch_size):\n",
        "        minibatch = random.sample(self.memory, batch_size)\n",
        "        for state, action, reward, next_state, done in minibatch:\n",
        "            target = reward\n",
        "            if not done:\n",
        "                target = (reward + self.gamma * np.amax(self.model.predict(next_state)[0]))\n",
        "            target_f = self.model.predict(state)\n",
        "            target_f[0][action] = target\n",
        "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ],
      "metadata": {
        "id": "Vro1NYoNLfgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Learner:\n",
        "  def __init__(self, agent, env):\n",
        "    self.agent = agent\n",
        "    self.env = env\n",
        "\n",
        "  def fit(self, epoch_n):\n",
        "    state = self.env.reset()\n",
        "    for _ in range(epoch_n):\n",
        "      action = self.agent.act(state)\n",
        "      new_state, reward, done = self.env.step(action)\n",
        "      self.agent.remember(state, action, reward, new_state, done)\n",
        "      self.agent.replay(10)"
      ],
      "metadata": {
        "id": "o4IFQheS2brH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('data.csv')\n",
        "data = normalize(data)\n",
        "forest = RandomForestClassifier()\n",
        "detector = Detector(forest)\n",
        "detector.fit(data)"
      ],
      "metadata": {
        "id": "Vvri8PLvh_2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = Enviroment(detector, None)\n",
        "agent = DQLAgent(7)\n",
        "learner = Learner(agent, env)\n",
        "learner.fit(100)"
      ],
      "metadata": {
        "id": "laA38_Pei7o0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class CEM():\n",
        "#     def __init__(self, state_n, action_n):\n",
        "#       self.state_n = state_n\n",
        "#       self.action_n = action_n\n",
        "#       self.policy = np.ones((self.state_n, self.action_n)) / self.action_n\n",
        "\n",
        "#     def get_action(self, state):\n",
        "#       return int(np.random.choice(np.arange(self.action_n), p=self.policy[state]))\n",
        "\n",
        "#     def update_policy(self, elite_tr):\n",
        "#       pre_policy = np.zeros((state_n, action_n))\n",
        "\n",
        "#       for tr in elite_tr:\n",
        "#         for state, action in zip(tr['states'], tr['actions']):\n",
        "#           pre_policy[state][action] += 1\n",
        "\n",
        "#           for state in range(self.action_n):\n",
        "#             if sum(pre_policy[state]) == 0:\n",
        "#               self.policy[state] = np.ones(self.action_n) / self.action_n\n",
        "#             else:\n",
        "#               self.policy[state] = pre_policy[state] / sum(pre_policy[state])"
      ],
      "metadata": {
        "id": "Y2yfqwRcXKqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class CEMAgent():\n",
        "#     def __init__(self, env, agent_logic):\n",
        "#       self.env = env\n",
        "#       self.agent_logic = agent_logic\n",
        "\n",
        "#     def get_state(self):\n",
        "#       return None\n",
        "\n",
        "#     def get_trajectory(self, tr_len):\n",
        "#       tr = {'states': [], 'actions': [], 'total_reward': 0}\n",
        "\n",
        "#       obs = self.env.reset()\n",
        "#       state = get_state(obs)\n",
        "#       tr['states'].append(state)\n",
        "\n",
        "#       for _ in range(tr_len):\n",
        "#           action = self.agent_logic.get_action(state)\n",
        "#           tr['actions'].append(action)\n",
        "\n",
        "#           obs, reward, done = env.step(action)\n",
        "#           state = get_state(obs)\n",
        "#           tr['total_reward'] += reward\n",
        "\n",
        "#           if done:\n",
        "#               break\n",
        "\n",
        "#           tr['states'].append(state)\n",
        "\n",
        "#         return tr\n",
        "#       def get_elite_trajectories(self, trajectories, q):\n",
        "#           total = [tr['total_reward'] for tr in trajectories]\n",
        "#           quantile = np.quantile(total, q=q)\n",
        "#           return [tr for tr in trajectories if tr['total_reward'] > quantile]\n",
        "\n",
        "#       def fit(self, episode_n, trajectory_n, trajectory_len, q):\n",
        "#           for _ in range(episode_n):\n",
        "#               trajectories = [get_trajectory(trajectory_len) for _ in range(trajectory_n)]\n",
        "\n",
        "#               mean_total = np.mean([trajectory['total_reward'] for trajectory in trajectories])\n",
        "#               print(mean_total)\n",
        "#               elite = get_elite_trajectories(trajectories, q)\n",
        "\n",
        "#               if len(elite) > 0:\n",
        "#                   self.agent_logic.update_policy(elite)"
      ],
      "metadata": {
        "id": "581vBZbWe6wv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}